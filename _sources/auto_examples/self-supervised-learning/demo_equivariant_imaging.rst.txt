
.. DO NOT EDIT.
.. THIS FILE WAS AUTOMATICALLY GENERATED BY SPHINX-GALLERY.
.. TO MAKE CHANGES, EDIT THE SOURCE PYTHON FILE:
.. "auto_examples/self-supervised-learning/demo_equivariant_imaging.py"
.. LINE NUMBERS ARE GIVEN BELOW.

.. only:: html

    .. note::
        :class: sphx-glr-download-link-note

        :ref:`Go to the end <sphx_glr_download_auto_examples_self-supervised-learning_demo_equivariant_imaging.py>`
        to download the full example code.

.. rst-class:: sphx-glr-example-title

.. _sphx_glr_auto_examples_self-supervised-learning_demo_equivariant_imaging.py:


Self-supervised learning with Equivariant Imaging for MRI.
====================================================================================================

This example shows you how to train a reconstruction network for an MRI inverse problem on a fully self-supervised way, i.e., using measurement data only.

The equivariant imaging loss is presented in `"Equivariant Imaging: Learning Beyond the Range Space"
<http://openaccess.thecvf.com/content/ICCV2021/papers/Chen_Equivariant_Imaging_Learning_Beyond_the_Range_Space_ICCV_2021_paper.pdf>`_.

.. GENERATED FROM PYTHON SOURCE LINES 11-21

.. code-block:: Python


    import deepinv as dinv
    from torch.utils.data import DataLoader
    import torch
    from pathlib import Path
    from torchvision import transforms
    from deepinv.optim.prior import PnP
    from deepinv.utils.demo import load_dataset, load_degradation
    from deepinv.models.utils import get_weights_url








.. GENERATED FROM PYTHON SOURCE LINES 22-25

Setup paths for data loading and results.
---------------------------------------------------------------


.. GENERATED FROM PYTHON SOURCE LINES 25-38

.. code-block:: Python


    BASE_DIR = Path(".")
    ORIGINAL_DATA_DIR = BASE_DIR / "datasets"
    DATA_DIR = BASE_DIR / "measurements"
    RESULTS_DIR = BASE_DIR / "results"
    DEG_DIR = BASE_DIR / "degradations"
    CKPT_DIR = BASE_DIR / "ckpts"

    # Set the global random seed from pytorch to ensure reproducibility of the example.
    torch.manual_seed(0)

    device = dinv.utils.get_freer_gpu() if torch.cuda.is_available() else "cpu"








.. GENERATED FROM PYTHON SOURCE LINES 39-48

Load base image datasets and degradation operators.
----------------------------------------------------------------------------------
In this example, we use a subset of the single-coil `FastMRI dataset <https://fastmri.org/>`_
as the base image dataset. It consists of 973 knee images of size 320x320.

.. note::

      We reduce to the size to 128x128 for faster training in the demo.


.. GENERATED FROM PYTHON SOURCE LINES 48-62

.. code-block:: Python


    operation = "MRI"
    train_dataset_name = "fastmri_knee_singlecoil"
    img_size = 128

    transform = transforms.Compose([transforms.Resize(img_size)])

    train_dataset = load_dataset(
        train_dataset_name, ORIGINAL_DATA_DIR, transform, train=True
    )
    test_dataset = load_dataset(
        train_dataset_name, ORIGINAL_DATA_DIR, transform, train=False
    )





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    Downloading datasets/fastmri_knee_singlecoil.pt
      0%|          | 0.00/399M [00:00<?, ?iB/s]      1%|          | 4.12M/399M [00:00<00:09, 41.2MiB/s]      3%|▎         | 11.2M/399M [00:00<00:06, 58.4MiB/s]      5%|▍         | 18.4M/399M [00:00<00:05, 64.6MiB/s]      6%|▋         | 25.6M/399M [00:00<00:05, 67.5MiB/s]      8%|▊         | 32.9M/399M [00:00<00:05, 69.6MiB/s]     10%|█         | 40.0M/399M [00:00<00:05, 70.2MiB/s]     12%|█▏        | 47.3M/399M [00:00<00:04, 71.0MiB/s]     14%|█▎        | 54.6M/399M [00:00<00:04, 71.6MiB/s]     16%|█▌        | 61.9M/399M [00:00<00:04, 72.0MiB/s]     17%|█▋        | 69.2M/399M [00:01<00:04, 72.4MiB/s]     19%|█▉        | 76.4M/399M [00:01<00:04, 72.2MiB/s]     21%|██        | 83.7M/399M [00:01<00:04, 72.1MiB/s]     23%|██▎       | 90.9M/399M [00:01<00:04, 72.0MiB/s]     25%|██▍       | 98.1M/399M [00:01<00:04, 72.1MiB/s]     26%|██▋       | 105M/399M [00:01<00:04, 71.8MiB/s]      28%|██▊       | 113M/399M [00:01<00:03, 71.7MiB/s]     30%|███       | 120M/399M [00:01<00:03, 71.6MiB/s]     32%|███▏      | 127M/399M [00:01<00:03, 71.7MiB/s]     34%|███▎      | 134M/399M [00:01<00:03, 72.0MiB/s]     36%|███▌      | 141M/399M [00:02<00:03, 72.4MiB/s]     37%|███▋      | 149M/399M [00:02<00:03, 72.5MiB/s]     39%|███▉      | 156M/399M [00:02<00:03, 72.6MiB/s]     41%|████      | 163M/399M [00:02<00:03, 72.5MiB/s]     43%|████▎     | 171M/399M [00:02<00:03, 72.3MiB/s]     45%|████▍     | 178M/399M [00:02<00:03, 72.1MiB/s]     46%|████▋     | 185M/399M [00:02<00:02, 71.9MiB/s]     48%|████▊     | 192M/399M [00:02<00:02, 72.0MiB/s]     50%|█████     | 199M/399M [00:02<00:02, 71.2MiB/s]     52%|█████▏    | 207M/399M [00:02<00:02, 70.8MiB/s]     54%|█████▎    | 214M/399M [00:03<00:02, 70.9MiB/s]     55%|█████▌    | 221M/399M [00:03<00:02, 70.7MiB/s]     57%|█████▋    | 228M/399M [00:03<00:02, 70.9MiB/s]     59%|█████▉    | 235M/399M [00:03<00:02, 71.0MiB/s]     61%|██████    | 242M/399M [00:03<00:02, 71.2MiB/s]     63%|██████▎   | 249M/399M [00:03<00:02, 71.2MiB/s]     64%|██████▍   | 256M/399M [00:03<00:02, 70.9MiB/s]     66%|██████▌   | 264M/399M [00:03<00:01, 70.8MiB/s]     68%|██████▊   | 271M/399M [00:03<00:01, 70.5MiB/s]     70%|██████▉   | 278M/399M [00:03<00:01, 70.2MiB/s]     71%|███████▏  | 285M/399M [00:04<00:01, 70.6MiB/s]     73%|███████▎  | 292M/399M [00:04<00:01, 70.5MiB/s]     75%|███████▌  | 299M/399M [00:04<00:01, 70.6MiB/s]     77%|███████▋  | 306M/399M [00:04<00:01, 70.7MiB/s]     79%|███████▊  | 313M/399M [00:04<00:01, 71.0MiB/s]     80%|████████  | 320M/399M [00:04<00:01, 71.1MiB/s]     82%|████████▏ | 327M/399M [00:04<00:00, 71.1MiB/s]     84%|████████▍ | 335M/399M [00:04<00:00, 71.2MiB/s]     86%|████████▌ | 342M/399M [00:04<00:00, 70.7MiB/s]     88%|████████▊ | 349M/399M [00:04<00:00, 70.0MiB/s]     89%|████████▉ | 356M/399M [00:05<00:00, 70.1MiB/s]     91%|█████████ | 363M/399M [00:05<00:00, 70.4MiB/s]     93%|█████████▎| 370M/399M [00:05<00:00, 70.8MiB/s]     95%|█████████▍| 377M/399M [00:05<00:00, 70.8MiB/s]     96%|█████████▋| 384M/399M [00:05<00:00, 70.8MiB/s]     98%|█████████▊| 391M/399M [00:05<00:00, 70.8MiB/s]    100%|██████████| 399M/399M [00:05<00:00, 70.9MiB/s]




.. GENERATED FROM PYTHON SOURCE LINES 63-67

Generate a dataset of knee images and load it.
----------------------------------------------------------------------------------



.. GENERATED FROM PYTHON SOURCE LINES 67-97

.. code-block:: Python


    mask = load_degradation("mri_mask_128x128.npy", ORIGINAL_DATA_DIR)

    # defined physics
    physics = dinv.physics.MRI(mask=mask, device=device)

    # Use parallel dataloader if using a GPU to fasten training,
    # otherwise, as all computes are on CPU, use synchronous data loading.
    num_workers = 4 if torch.cuda.is_available() else 0
    n_images_max = (
        900 if torch.cuda.is_available() else 5
    )  # number of images used for training
    # (the dataset has up to 973 images, however here we use only 900)

    my_dataset_name = "demo_equivariant_imaging"
    measurement_dir = DATA_DIR / train_dataset_name / operation
    deepinv_datasets_path = dinv.datasets.generate_dataset(
        train_dataset=train_dataset,
        test_dataset=test_dataset,
        physics=physics,
        device=device,
        save_dir=measurement_dir,
        train_datapoints=n_images_max,
        num_workers=num_workers,
        dataset_filename=str(my_dataset_name),
    )

    train_dataset = dinv.datasets.HDF5Dataset(path=deepinv_datasets_path, train=True)
    test_dataset = dinv.datasets.HDF5Dataset(path=deepinv_datasets_path, train=False)





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    mri_mask_128x128.npy degradation downloaded in datasets
    Dataset has been saved in measurements/fastmri_knee_singlecoil/MRI




.. GENERATED FROM PYTHON SOURCE LINES 98-103

Set up the reconstruction network
---------------------------------------------------------------

As a reconstruction network, we use an unrolled network (half-quadratic splitting)
with a trainable denoising prior based on the DnCNN architecture.

.. GENERATED FROM PYTHON SOURCE LINES 103-148

.. code-block:: Python


    # Select the data fidelity term
    data_fidelity = dinv.optim.L2()
    n_channels = 2  # real + imaginary parts

    # If the prior dict value is initialized with a table of length max_iter, then a distinct model is trained for each
    # iteration. For fixed trained model prior across iterations, initialize with a single model.
    prior = PnP(
        denoiser=dinv.models.DnCNN(
            in_channels=n_channels,
            out_channels=n_channels,
            pretrained=None,
            train=True,
            depth=7,
        ).to(device)
    )

    # Unrolled optimization algorithm parameters
    max_iter = 3  # number of unfolded layers
    lamb = [1.0] * max_iter  # initialization of the regularization parameter
    stepsize = [1.0] * max_iter  # initialization of the step sizes.
    sigma_denoiser = [0.01] * max_iter  # initialization of the denoiser parameters
    params_algo = {  # wrap all the restoration parameters in a 'params_algo' dictionary
        "stepsize": stepsize,
        "g_param": sigma_denoiser,
        "lambda": lamb,
    }

    trainable_params = [
        "lambda",
        "stepsize",
        "g_param",
    ]  # define which parameters from 'params_algo' are trainable

    # Define the unfolded trainable model.
    model = dinv.unfolded.unfolded_builder(
        "HQS",
        params_algo=params_algo,
        trainable_params=trainable_params,
        data_fidelity=data_fidelity,
        max_iter=max_iter,
        prior=prior,
    )









.. GENERATED FROM PYTHON SOURCE LINES 149-162

Set up the training parameters
--------------------------------------------
We choose a self-supervised training scheme with two losses: the measurement consistency loss (MC)
and the equivariant imaging loss (EI).
The EI loss requires a group of transformations to be defined. The forward model `should not be equivariant to
these transformations <https://www.jmlr.org/papers/v24/22-0315.html>`_.
Here we use the group of 4 rotations of 90 degrees, as the accelerated MRI acquisition is
not equivariant to rotations (while it is equivariant to translations).

.. note::

      We use a pretrained model to reduce training time. You can get the same results by training from scratch
      for 150 epochs.

.. GENERATED FROM PYTHON SOURCE LINES 162-187

.. code-block:: Python


    epochs = 1  # choose training epochs
    learning_rate = 5e-4
    batch_size = 16 if torch.cuda.is_available() else 1

    # choose self-supervised training losses
    # generates 4 random rotations per image in the batch
    losses = [dinv.loss.MCLoss(), dinv.loss.EILoss(dinv.transform.Rotate(4))]

    # choose optimizer and scheduler
    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-8)
    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=int(epochs * 0.8) + 1)

    # start with a pretrained model to reduce training time
    file_name = "new_demo_ei_ckp_150_v3.pth"
    url = get_weights_url(model_name="demo", file_name=file_name)
    ckpt = torch.hub.load_state_dict_from_url(
        url,
        map_location=lambda storage, loc: storage,
        file_name=file_name,
    )
    # load a checkpoint to reduce training time
    model.load_state_dict(ckpt["state_dict"])
    optimizer.load_state_dict(ckpt["optimizer"])





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    Downloading: "https://huggingface.co/deepinv/demo/resolve/main/new_demo_ei_ckp_150_v3.pth?download=true" to /home/runner/.cache/torch/hub/checkpoints/new_demo_ei_ckp_150_v3.pth
      0%|          | 0.00/2.17M [00:00<?, ?B/s]    100%|██████████| 2.17M/2.17M [00:00<00:00, 32.0MB/s]




.. GENERATED FROM PYTHON SOURCE LINES 188-191

Train the network
--------------------------------------------


.. GENERATED FROM PYTHON SOURCE LINES 191-223

.. code-block:: Python



    verbose = True  # print training information
    wandb_vis = False  # plot curves and images in Weight&Bias

    train_dataloader = DataLoader(
        train_dataset, batch_size=batch_size, num_workers=num_workers, shuffle=True
    )
    test_dataloader = DataLoader(
        test_dataset, batch_size=batch_size, num_workers=num_workers, shuffle=False
    )

    # Initialize the trainer
    trainer = dinv.Trainer(
        model,
        physics=physics,
        epochs=epochs,
        scheduler=scheduler,
        losses=losses,
        optimizer=optimizer,
        train_dataloader=train_dataloader,
        plot_images=True,
        device=device,
        save_path=str(CKPT_DIR / operation),
        verbose=verbose,
        wandb_vis=wandb_vis,
        show_progress_bar=False,  # disable progress bar for better vis in sphinx gallery.
        ckp_interval=10,
    )

    model = trainer.train()




.. image-sg:: /auto_examples/self-supervised-learning/images/sphx_glr_demo_equivariant_imaging_001.png
   :alt: Backprojection, Output, Target
   :srcset: /auto_examples/self-supervised-learning/images/sphx_glr_demo_equivariant_imaging_001.png
   :class: sphx-glr-single-img


.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    The model has 187019 trainable parameters
    Train epoch 0: MCLoss=0.0, EILoss=0.0, TotalLoss=0.0, PSNR=36.644




.. GENERATED FROM PYTHON SOURCE LINES 224-228

Test the network
--------------------------------------------



.. GENERATED FROM PYTHON SOURCE LINES 228-230

.. code-block:: Python


    trainer.test(test_dataloader)



.. image-sg:: /auto_examples/self-supervised-learning/images/sphx_glr_demo_equivariant_imaging_002.png
   :alt: No learning, Recons., GT
   :srcset: /auto_examples/self-supervised-learning/images/sphx_glr_demo_equivariant_imaging_002.png
   :class: sphx-glr-single-img


.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    Test PSNR: No learning rec.: 29.389+-3.411 | Model: 36.924+-2.392. 

    (36.923609511493005, 2.3915292985620225, 29.388799536718082, 3.4114185158882959)




.. rst-class:: sphx-glr-timing

   **Total running time of the script:** (0 minutes 27.215 seconds)


.. _sphx_glr_download_auto_examples_self-supervised-learning_demo_equivariant_imaging.py:

.. only:: html

  .. container:: sphx-glr-footer sphx-glr-footer-example

    .. container:: sphx-glr-download sphx-glr-download-jupyter

      :download:`Download Jupyter notebook: demo_equivariant_imaging.ipynb <demo_equivariant_imaging.ipynb>`

    .. container:: sphx-glr-download sphx-glr-download-python

      :download:`Download Python source code: demo_equivariant_imaging.py <demo_equivariant_imaging.py>`


.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_
