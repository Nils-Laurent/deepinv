
.. DO NOT EDIT.
.. THIS FILE WAS AUTOMATICALLY GENERATED BY SPHINX-GALLERY.
.. TO MAKE CHANGES, EDIT THE SOURCE PYTHON FILE:
.. "auto_examples/self-supervised-learning/demo_ei_transforms.py"
.. LINE NUMBERS ARE GIVEN BELOW.

.. only:: html

    .. note::
        :class: sphx-glr-download-link-note

        :ref:`Go to the end <sphx_glr_download_auto_examples_self-supervised-learning_demo_ei_transforms.py>`
        to download the full example code.

.. rst-class:: sphx-glr-example-title

.. _sphx_glr_auto_examples_self-supervised-learning_demo_ei_transforms.py:


Image transformations for Equivariant Imaging
=============================================

This example demonstrates various geometric image transformations
implemented in ``deepinv`` that can be used in Equivariant Imaging (EI)
for self-supervised learning:

-  Shift: integer pixel 2D shift;
-  Rotate: 2D image rotation;
-  Scale: continuous 2D image downscaling;
-  Euclidean: includes continuous translation, rotation, and reflection,
   forming the group :math:`\mathbb{E}(2)`;
-  Similarity: as above but includes scale, forming the group
   :math:`\text{S}(2)`;
-  Affine: as above but includes shear effects, forming the group
   :math:`\text{Aff}(3)`;
-  Homography: as above but includes perspective (i.e pan and tilt)
   effects, forming the group :math:`\text{PGL}(3)`;
-  PanTiltRotate: pure 3D camera rotation i.e pan, tilt and 2D image
   rotation.

These were proposed in the papers:

-  ``Shift``, ``Rotate``: `Chen et al., Equivariant Imaging: Learning
   Beyond the Range
   Space <https://openaccess.thecvf.com/content/ICCV2021/papers/Chen_Equivariant_Imaging_Learning_Beyond_the_Range_Space_ICCV_2021_paper.pdf>`__
-  ``Scale``: `Scanvic et al., Self-Supervised Learning for Image
   Super-Resolution and Deblurring <https://arxiv.org/abs/2312.11232>`__
-  ``Homography`` and the projective geometry framework: `Wang et al.,
   Perspective-Equivariant Imaging: an Unsupervised Framework for
   Multispectral Pansharpening <https://arxiv.org/abs/2403.09327>`__

.. GENERATED FROM PYTHON SOURCE LINES 35-46

.. code-block:: Python


    import deepinv as dinv
    import torch
    from torch.utils.data import DataLoader, random_split
    from torchvision.datasets import ImageFolder
    from torchvision.transforms import Compose, ToTensor, CenterCrop, Resize
    from torchvision.datasets.utils import download_and_extract_archive

    device = dinv.utils.get_freer_gpu() if torch.cuda.is_available() else "cpu"









.. GENERATED FROM PYTHON SOURCE LINES 47-50

Define transforms. For the transforms that involve 3D camera rotation
(i.e pan or tilt), we limit ``theta_max`` for display.


.. GENERATED FROM PYTHON SOURCE LINES 50-63

.. code-block:: Python


    transforms = [
        dinv.transform.Shift(),
        dinv.transform.Rotate(),
        dinv.transform.Scale(),
        dinv.transform.Homography(theta_max=10),
        dinv.transform.projective.Euclidean(),
        dinv.transform.projective.Similarity(),
        dinv.transform.projective.Affine(),
        dinv.transform.projective.PanTiltRotate(theta_max=10),
    ]









.. GENERATED FROM PYTHON SOURCE LINES 64-68

Plot transforms on a sample image. Note that, during training, we never
have access to these ground truth images ``x``, only partial and noisy
measurements ``y``.


.. GENERATED FROM PYTHON SOURCE LINES 68-76

.. code-block:: Python


    x = dinv.utils.load_url_image(dinv.utils.demo.get_image_url("celeba_example.jpg"))
    dinv.utils.plot(
        [x] + [t(x) for t in transforms],
        ["Orig"] + [t.__class__.__name__ for t in transforms],
    )





.. image-sg:: /auto_examples/self-supervised-learning/images/sphx_glr_demo_ei_transforms_001.png
   :alt: Orig, Shift, Rotate, Scale, Homography, Euclidean, Similarity, Affine, PanTiltRotate
   :srcset: /auto_examples/self-supervised-learning/images/sphx_glr_demo_ei_transforms_001.png
   :class: sphx-glr-single-img





.. GENERATED FROM PYTHON SOURCE LINES 77-84

Now, we run an inpainting experiment to reconstruct images from images
masked with a random mask, without ground truth, using EI. For this
example we use the Urban100 images of natural urban scenes. As these
scenes are imaged with a camera free to move and rotate in the world,
all of the above transformations are valid invariances that we can
impose on the unknown image set :math:`x\in X`.


.. GENERATED FROM PYTHON SOURCE LINES 84-106

.. code-block:: Python


    physics = dinv.physics.Inpainting((3, 256, 256), mask=0.6, device=device)

    download_and_extract_archive(
        "https://huggingface.co/datasets/eugenesiow/Urban100/resolve/main/data/Urban100_HR.tar.gz?download=true",
        "Urban100",
        filename="Urban100_HR.tar.gz",
        md5="65d9d84a34b72c6f7ca1e26a12df1e4c",
    )

    train_dataset, test_dataset = random_split(
        ImageFolder(
            "Urban100", transform=Compose([ToTensor(), Resize(256), CenterCrop(256)])
        ),
        (0.8, 0.2),
    )

    train_dataloader, test_dataloader = DataLoader(train_dataset, shuffle=True), DataLoader(
        test_dataset
    )






.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    Downloading https://cdn-lfs.huggingface.co/datasets/eugenesiow/Urban100/25b929945e053de19bf8f575bd0821abab1eac7f5f6c5c7980221d8ed13066c6?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27Urban100_HR.tar.gz%3B+filename%3D%22Urban100_HR.tar.gz%22%3B&response-content-type=application%2Fgzip&Expires=1719846664&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTcxOTg0NjY2NH19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5odWdnaW5nZmFjZS5jby9kYXRhc2V0cy9ldWdlbmVzaW93L1VyYmFuMTAwLzI1YjkyOTk0NWUwNTNkZTE5YmY4ZjU3NWJkMDgyMWFiYWIxZWFjN2Y1ZjZjNWM3OTgwMjIxZDhlZDEzMDY2YzY%7EcmVzcG9uc2UtY29udGVudC1kaXNwb3NpdGlvbj0qJnJlc3BvbnNlLWNvbnRlbnQtdHlwZT0qIn1dfQ__&Signature=m1MCxyjoy6Zyj4v1goYbDUl%7EGUCRKzEyQowp%7EzpskhZ%7EG13ipYM5gDJgJSe%7EW0DElvLU1y13ZRDX2wyogyV%7EzDoFyiFeMufWWyLyg50Fk8B-sd8IJjCoPytxvpu6p-VnN6ad%7E1upfQLyoxIH1-CnxHqHKqMWI325GYfhZcgq39EnVOpuo0hoDhkNxmn08Xy0LjjIPksYnWzBOQixE3al6ODkw47d8W8STsckJiLXCdAJWYmELPr08GKUSzh%7E3MO5f8%7EZLm%7E4-cfGU1FVcpSQl9hW2LpQrsdFxHTkLVDFYVRBpVrchm534Hxa2UIeJQB9Jo57eoPMqQKg1nlp3NNETg__&Key-Pair-Id=K3ESJI6DHPFC7 to Urban100/Urban100_HR.tar.gz
      0%|          | 0/135388067 [00:00<?, ?it/s]     18%|█▊        | 24248320/135388067 [00:00<00:00, 242437189.98it/s]     44%|████▎     | 59146240/135388067 [00:00<00:00, 305028949.13it/s]     72%|███████▏  | 97320960/135388067 [00:00<00:00, 339979335.59it/s]    100%|█████████▉| 134905856/135388067 [00:00<00:00, 354069699.42it/s]    100%|██████████| 135388067/135388067 [00:00<00:00, 337082955.49it/s]
    Extracting Urban100/Urban100_HR.tar.gz to Urban100




.. GENERATED FROM PYTHON SOURCE LINES 107-114

For training, use a small UNet, Adam optimizer, EI loss with homography
transform, and the ``deepinv.Trainer`` functionality:

.. note::

      We only train for a single epoch in the demo, but it is recommended to train multiple epochs in practice.


.. GENERATED FROM PYTHON SOURCE LINES 114-142

.. code-block:: Python


    model = dinv.models.UNet(
        in_channels=3, out_channels=3, scales=2, circular_padding=True, batch_norm=False
    ).to(device)

    losses = [
        dinv.loss.MCLoss(),
        dinv.loss.EILoss(dinv.transform.Homography(theta_max=10, device=device)),
    ]

    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-8)

    model = dinv.Trainer(
        model=model,
        physics=physics,
        online_measurements=True,
        train_dataloader=train_dataloader,
        eval_dataloader=test_dataloader,
        epochs=1,
        losses=losses,
        optimizer=optimizer,
        verbose=True,
        show_progress_bar=False,
        save_path=None,
        device=device,
    ).train()






.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    The model has 444867 trainable parameters
    Eval epoch 0: PSNR=9.952
    Train epoch 0: MCLoss=0.008, EILoss=0.023, TotalLoss=0.03, PSNR=10.377




.. GENERATED FROM PYTHON SOURCE LINES 143-146

Show results of a pretrained model trained using a larger UNet for 40
epochs:


.. GENERATED FROM PYTHON SOURCE LINES 146-163

.. code-block:: Python


    model = dinv.models.UNet(
        in_channels=3, out_channels=3, scales=3, circular_padding=True, batch_norm=False
    ).to(device)

    ckpt = torch.hub.load_state_dict_from_url(
        dinv.models.utils.get_weights_url("ei", "Urban100_inpainting_homography_model.pth"),
        map_location=device,
    )

    model.load_state_dict(ckpt["state_dict"])

    x, _ = next(iter(train_dataloader))
    y = physics(x)
    x_hat = model(y)

    dinv.utils.plot([x, y, x_hat], ["x", "y", "reconstruction"])



.. image-sg:: /auto_examples/self-supervised-learning/images/sphx_glr_demo_ei_transforms_002.png
   :alt: x, y, reconstruction
   :srcset: /auto_examples/self-supervised-learning/images/sphx_glr_demo_ei_transforms_002.png
   :class: sphx-glr-single-img


.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    Downloading: "https://huggingface.co/deepinv/ei/resolve/main/Urban100_inpainting_homography_model.pth?download=true" to /home/runner/.cache/torch/hub/checkpoints/Urban100_inpainting_homography_model.pth
      0%|          | 0.00/7.90M [00:00<?, ?B/s]     79%|███████▉  | 6.25M/7.90M [00:00<00:00, 65.3MB/s]    100%|██████████| 7.90M/7.90M [00:00<00:00, 76.4MB/s]





.. rst-class:: sphx-glr-timing

   **Total running time of the script:** (2 minutes 14.230 seconds)


.. _sphx_glr_download_auto_examples_self-supervised-learning_demo_ei_transforms.py:

.. only:: html

  .. container:: sphx-glr-footer sphx-glr-footer-example

    .. container:: sphx-glr-download sphx-glr-download-jupyter

      :download:`Download Jupyter notebook: demo_ei_transforms.ipynb <demo_ei_transforms.ipynb>`

    .. container:: sphx-glr-download sphx-glr-download-python

      :download:`Download Python source code: demo_ei_transforms.py <demo_ei_transforms.py>`


.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_
